{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1262b9e-8a75-44d4-a103-443d742ed5b6",
   "metadata": {},
   "source": [
    "Демонстрационный ноут по иерархической тематической модели TreeProfitTM на основе \"плоской\" версии ProfitTM.\n",
    "\n",
    "Плюсы:\n",
    "1) Автоматическое определение количества оптимальных тем\n",
    "2) Поддержка иерархической тематизации\n",
    "3) Высокая интерпретируемость и декорреляция топиков\n",
    "4) Скорость инференса\n",
    "5) Гибкость настроек\n",
    "6) Не требует много видеопамяти (можно относительно быстро натренировать даже на CPU)\n",
    "\n",
    "Минусы:\n",
    "1) Необходимость использовать нейронки на стадии тренировки модели\n",
    "2) Аггломеративная кластеризация медленная и требует много памяти (можно настроить)\n",
    "\n",
    "Принцип работы ProfitTM:\n",
    "\n",
    "Тренировка:\n",
    "1) Разбиваем вектора текстов с помощью аггломеративной кластеризации\n",
    "2) Выполняем слияние мелких кластеров с крупными на основе расстояний\n",
    "3) Тренируем нейронку-классификатор с CenterLoss\n",
    "4) Выполняем слияние мелких кластеров с крупными на основе размеров кластеров\n",
    "5) Тренируем нейронку-классификатор с CenterLoss\n",
    "6) Тренируем быстрый классификатор на получившихся классах (на стадии инференса нейронки не используются)\n",
    "   \n",
    "Инференс:\n",
    "1) Быстрая классификация исходных векторов без нейронок\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1dd380-cf79-45a3-a45d-99ba362f62b9",
   "metadata": {},
   "source": [
    "Создайте виртуальную среду python=3.10 и установите необходимые пакеты:\n",
    "\n",
    "conda create -n profittm python=3.10\r\n",
    "conda activateprofittmv#\r\n",
    "pip3 install torch==1.13.0+cu117 torchvision==0.14.0+cu117 torchaudio===0.13.0+cu117 -f https://download.pytorch.org/whl/cu117/torch_stable.ht\n",
    "pip3 install torch==1.13ml\r\n",
    "pip install -r requirements.txt\r\n",
    "python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf84916-05be-401d-82f4-64b69e6534d4",
   "metadata": {},
   "source": [
    "Импорт зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0556cd-70ce-456a-9e0c-9af02cd147d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd.function import Function\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from gensim.models import Word2Vec\n",
    "from copy import deepcopy\n",
    "import gensim.downloader as api\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from copy import deepcopy\n",
    "from pprint import pprint\n",
    "from scipy.spatial.distance import euclidean\n",
    "from torch.jit import isinstance\n",
    "\n",
    "import networkx as nx\n",
    "from numpy import dtype\n",
    "import uuid\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8041826-ba54-4eac-8842-b1301b16e7b4",
   "metadata": {},
   "source": [
    "utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d428a1-b61b-44f2-832c-515e7fb969a5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def save(obj, path, verbose=True):\n",
    "    if verbose:\n",
    "        print(\"Saving object to {}\".format(path))\n",
    "\n",
    "    with open(path, \"wb\") as obj_file:\n",
    "        pickle.dump( obj, obj_file, protocol=pickle.HIGHEST_PROTOCOL )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Object saved to {}\".format(path))\n",
    "    pass\n",
    "\n",
    "def load(path, verbose=True):\n",
    "    if verbose:\n",
    "        print(\"Loading object from {}\".format(path))\n",
    "    with open(path, \"rb\") as obj_file:\n",
    "        obj = pickle.load(obj_file)\n",
    "    if verbose:\n",
    "        print(\"Object loaded from {}\".format(path))\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c13a2b-5f91-4e61-8458-3830e8f7edb5",
   "metadata": {},
   "source": [
    "CenterLossCompressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3ad5cf-4c19-466c-b046-19634d7ba496",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CenterLoss(nn.Module):\n",
    "    def __init__(self, num_classes, feat_dim, size_average=True):\n",
    "        super(CenterLoss, self).__init__()\n",
    "        self.centers = nn.Parameter(torch.randn(num_classes, feat_dim))\n",
    "        self.centerlossfunc = CenterlossFunc.apply\n",
    "        self.feat_dim = feat_dim\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, label, feat):\n",
    "        batch_size = feat.size(0)\n",
    "        feat = feat.view(batch_size, -1)\n",
    "        # To check the dim of centers and features\n",
    "        if feat.size(1) != self.feat_dim:\n",
    "            raise ValueError(\"Center's dim: {0} should be equal to input feature's \\\n",
    "                            dim: {1}\".format(self.feat_dim, feat.size(1)))\n",
    "        batch_size_tensor = feat.new_empty(1).fill_(\n",
    "            batch_size if self.size_average else 1)\n",
    "        loss = self.centerlossfunc(\n",
    "            feat, label, self.centers, batch_size_tensor)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class CenterlossFunc(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, feature, label, centers, batch_size):\n",
    "        ctx.save_for_backward(feature, label, centers, batch_size)\n",
    "        centers_batch = centers.index_select(0, label.long())\n",
    "        return (feature - centers_batch).pow(2).sum() / 2.0 / batch_size\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        feature, label, centers, batch_size = ctx.saved_tensors\n",
    "        centers_batch = centers.index_select(0, label.long())\n",
    "        diff = centers_batch - feature\n",
    "        # init every iteration\n",
    "        counts = centers.new_ones(centers.size(0))\n",
    "        ones = centers.new_ones(label.size(0))\n",
    "        grad_centers = centers.new_zeros(centers.size())\n",
    "\n",
    "        counts = counts.scatter_add_(0, label.long(), ones)\n",
    "        grad_centers.scatter_add_(\n",
    "            0, label.unsqueeze(1).expand(\n",
    "                feature.size()).long(), diff)\n",
    "        grad_centers = grad_centers / counts.view(-1, 1)\n",
    "        return - grad_output * diff / batch_size, None, grad_centers / batch_size, None\n",
    "\n",
    "\n",
    "class CenterLossNN(nn.Module):\n",
    "    def __init__(self, x_shape, n_classes, latent_dim):\n",
    "        super(CenterLossNN, self).__init__()\n",
    "        self.conv1_1 = nn.Conv2d(1, 32, kernel_size=2)\n",
    "        torch.nn.init.kaiming_normal_(self.conv1_1.weight)\n",
    "        self.bn_1 = nn.BatchNorm2d(32)\n",
    "        self.prelu1_1 = nn.PReLU()\n",
    "        self.conv1_2 = nn.Conv2d(32, 64, kernel_size=2)\n",
    "        torch.nn.init.kaiming_normal_(self.conv1_2.weight)\n",
    "        self.bn_2 = nn.BatchNorm2d(64)\n",
    "        self.do_1 = nn.Dropout2d(p=0.2)\n",
    "        self.prelu1_2 = nn.PReLU()\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=2)\n",
    "        torch.nn.init.kaiming_normal_(self.conv2_1.weight)\n",
    "        self.bn_3 = nn.BatchNorm2d(128)\n",
    "        self.prelu3_2 = nn.PReLU()\n",
    "        \n",
    "        self.preluip1 = nn.PReLU()\n",
    "        self.ip1 = nn.Linear(128 * (x_shape[2] - 3), latent_dim)\n",
    "        self.ip2 = nn.Linear(latent_dim, n_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.prelu1_1(self.bn_1(self.conv1_1(x)))\n",
    "        x = self.prelu1_2(self.do_1(self.bn_2(self.conv1_2(x))))\n",
    "        x = self.prelu3_2(self.bn_3(self.conv2_1(x)))\n",
    "        \n",
    "        x = torch.flatten(x, start_dim=1, end_dim=-1)\n",
    "        \n",
    "        ip1 = self.preluip1(self.ip1(x))\n",
    "        \n",
    "        ip2 = self.ip2(ip1)\n",
    "        ip2 = F.log_softmax(ip2, dim=1)\n",
    "        \n",
    "        return ip1, ip2\n",
    "\n",
    "class MLP_Network(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_layer_dim, hidden_layers_num=2, dropout_rate=0.2):\n",
    "        super( MLP_Network, self).__init__()\n",
    "        \n",
    "        \n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "        \n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_layer_dim),\n",
    "            nn.BatchNorm1d(hidden_layer_dim),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.input_layer.apply(init_weights)\n",
    "        \n",
    "\n",
    "        self.middle_layers = []\n",
    "        for i in range( hidden_layers_num ):\n",
    "            hidden_layer = nn.Sequential(\n",
    "                nn.Linear(hidden_layer_dim, hidden_layer_dim),\n",
    "                nn.BatchNorm1d(hidden_layer_dim),\n",
    "                nn.Dropout(p=dropout_rate),\n",
    "                nn.PReLU(),\n",
    "            )\n",
    "            hidden_layer.apply(init_weights)\n",
    "            self.middle_layers.append( hidden_layer )\n",
    "        self.middle_layers = nn.Sequential( * self.middle_layers )\n",
    "        \n",
    "        self.latent_output = nn.Sequential(\n",
    "            nn.Linear(hidden_layer_dim, hidden_layer_dim),\n",
    "            #nn.BatchNorm1d(hidden_layer_dim),\n",
    "            nn.PReLU(),\n",
    "        )\n",
    "        self.latent_output.apply(init_weights)\n",
    "        \n",
    "        self.classes_output = nn.Linear(hidden_layer_dim, output_dim, bias=True)\n",
    "        nn.init.kaiming_uniform_(self.classes_output.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.middle_layers(x)\n",
    "        \n",
    "        latent_features = self.latent_output(x)\n",
    "        \n",
    "        classes_output = self.classes_output(latent_features)\n",
    "        classes_output = F.log_softmax(classes_output, dim=1)\n",
    "        \n",
    "        \n",
    "        return latent_features, classes_output\n",
    "\n",
    "class CompressorTrainDataset(Dataset):\n",
    "    def __init__(self, x, y, device, stack_count):\n",
    "        \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.device = device\n",
    "        self.stack_count = stack_count\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, id):\n",
    "        \n",
    "        if self.stack_count > 1:\n",
    "            stacked_x = np.vstack([self.x[id] for i in range(self.stack_count)])\n",
    "            stacked_x = torch.Tensor(stacked_x)\n",
    "            stacked_x = torch.unsqueeze(stacked_x, dim=0)\n",
    "        else:\n",
    "            stacked_x = self.x[id]\n",
    "            stacked_x = torch.Tensor(stacked_x)\n",
    "\n",
    "        stacked_x = stacked_x.to(self.device)\n",
    "        \n",
    "        y = self.y[id]\n",
    "        y = torch.tensor(y, dtype=torch.int64)\n",
    "        y = y.to(self.device)\n",
    "        \n",
    "        return stacked_x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        data_length = len(self.y)\n",
    "        \n",
    "        return data_length\n",
    "\n",
    "class CompressorTestDataset(Dataset):\n",
    "    def __init__(self, x, device, stack_count):\n",
    "        \n",
    "        self.x = x\n",
    "        self.device = device\n",
    "        self.stack_count = stack_count\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, id):\n",
    "        \n",
    "        if self.stack_count > 1:\n",
    "            stacked_x = np.vstack([self.x[id] for i in range(self.stack_count)])\n",
    "            stacked_x = torch.Tensor(stacked_x)\n",
    "            stacked_x = torch.unsqueeze(stacked_x, dim=0)\n",
    "        else:\n",
    "            stacked_x = self.x[id]\n",
    "            stacked_x = torch.Tensor(stacked_x)\n",
    "        \n",
    "        stacked_x = stacked_x.to(self.device)\n",
    "        \n",
    "        return stacked_x\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        data_length = len(self.x)\n",
    "        \n",
    "        return data_length\n",
    "\n",
    "class CenterLossCompressor():\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.device = torch.device('cuda')\n",
    "\n",
    "        self.latent_dim = None\n",
    "        self.n_classes = None\n",
    "        self.stack_count = None\n",
    "\n",
    "        pass\n",
    "\n",
    "    def fit(self, x, y, validation_part=0.05,\n",
    "            batch_size=100, epochs=100, latent_dim=100, stack_count=1):\n",
    "        \n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.latent_dim = latent_dim\n",
    "        self.stack_count = stack_count\n",
    "\n",
    "        \"\"\"self.model = CenterLossNN(\n",
    "            x_shape=(len(x), self.stack_count, len(x[0])),\n",
    "            n_classes=self.n_classes,\n",
    "            latent_dim=latent_dim)\"\"\"\n",
    "        \n",
    "        self.model = MLP_Network(input_dim=len(x[0]), output_dim=self.n_classes, \n",
    "                                 hidden_layer_dim=latent_dim, \n",
    "                                 hidden_layers_num=2, \n",
    "                                 dropout_rate=0.05)\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.model.train()\n",
    "\n",
    "        loss_weight = 1\n",
    "        nllloss = nn.CrossEntropyLoss().to(self.device)\n",
    "        centerloss = CenterLoss(self.n_classes, self.latent_dim).to(self.device)\n",
    "        optimizer4nn = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        optimzer4center = optim.Adam(centerloss.parameters(), lr=0.5)\n",
    "        \n",
    "        train_dataset = CompressorTrainDataset(x, y, device=\"cuda\", stack_count=stack_count)\n",
    "        \n",
    "        compressor_batch_size = len(x) // 100 + 2\n",
    "        if compressor_batch_size > 256:\n",
    "            compressor_batch_size = 256\n",
    "        print(\"Compressor batch size: {}\".format(compressor_batch_size))\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=compressor_batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_x, batch_y in tqdm(train_dataloader, desc='CenterLossCompressor fit | Epoch {} of {}'.format(epoch+1, epochs)):\n",
    "                ip1, pred = self.model(batch_x)\n",
    "                loss = nllloss(pred, batch_y) + loss_weight * centerloss(batch_y, ip1)\n",
    "\n",
    "                optimizer4nn.zero_grad()\n",
    "                optimzer4center.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer4nn.step()\n",
    "                optimzer4center.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        pass\n",
    "\n",
    "    def predict(self, x, batch_size=100):\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        test_dataset = CompressorTestDataset(x, device=\"cuda\", stack_count=self.stack_count)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        features = []\n",
    "        for x in test_dataloader:\n",
    "            with torch.no_grad():\n",
    "                feats, labels = self.model(x)\n",
    "                feats = feats.data.cpu().numpy()\n",
    "                features.append(feats)\n",
    "                batch = x.to('cpu')\n",
    "                del batch\n",
    "        features = np.vstack(features)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d291258-4421-4dab-af29-7a1251e27e5d",
   "metadata": {},
   "source": [
    "StubClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c61c1e8-e07f-471e-813f-6b478c17aa1e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class StubClassifier():\n",
    "    \n",
    "    def __init__(self, stub_y):\n",
    "        \n",
    "        self.stub_y = stub_y\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \n",
    "        stub_predicts = [ self.stub_y for i in range(len(x)) ]\n",
    "        stub_predicts = np.array( stub_predicts )\n",
    "        \n",
    "        return stub_predicts\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c35fd6-fc1c-49f8-862b-7b26c9776b94",
   "metadata": {},
   "source": [
    "SimpleDataPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7d44af-fcd0-4384-b1f2-afa1da7de403",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class SimpleDataPreprocessor():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stop_words = ['that', 'a', 'to', 'of', 'which', 'and', 'while', 'in', 'for', 'those', 'their', 'these',\n",
    "                          'this', 'but', 'howev', 'it', 'also', 'the', 'onli', 'have', 'one', 't', 's', 'v', 'd', 'at', 'has', 'what']\n",
    "        self.stop_words = self.stop_words + list(set(stopwords.words('english')))\n",
    "        # for i in range(len(self.stop_words)):\n",
    "        #    self.stop_words[i] = Stemmer.Stemmer(\"english\").stemWord(self.stop_words[i])\n",
    "        self.stop_words = list(set(self.stop_words))\n",
    "        #self.translit = Transliterator()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        #self.uselessWords = []\n",
    "\n",
    "    def preproc_doc_string(self, sample):\n",
    "        doc_string = str(sample)\n",
    "        doc_string = doc_string.lower()\n",
    "\n",
    "        #doc_string = list(doc_string)\n",
    "        # for i in range(len(doc_string)):\n",
    "        #    doc_string[i] = self.translit.transliterate(doc_string[i])\n",
    "        #doc_string = \"\".join(doc_string)\n",
    "\n",
    "        doc_string = re.sub('[^A-Za-zА-Яа-я\\\\s\\t]+', ' ', doc_string)\n",
    "\n",
    "        doc_string = doc_string.split()\n",
    "        for i in range(len(doc_string)):\n",
    "            doc_string[i] = self.lemmatizer.lemmatize(doc_string[i])\n",
    "        doc_string = ' '.join(doc_string)\n",
    "\n",
    "        #doc_string = doc_string.split()\n",
    "        # for i in range(len(doc_string)):\n",
    "        #    doc_string[i] = Stemmer.Stemmer(\"english\").stemWord(doc_string[i])\n",
    "        #doc_string = \" \".join(doc_string)\n",
    "\n",
    "        # remove stopwords\n",
    "        doc_string = doc_string.split()\n",
    "        doc_string = ' '.join([i for i in doc_string if i not in self.stop_words])\n",
    "        #checkWords = deepcopy(doc_string)\n",
    "        # for word in checkWords:\n",
    "        #    if word in self.stop_words:\n",
    "        #        doc_string.remove(word)\n",
    "        #doc_string = \" \".join(doc_string)\n",
    "\n",
    "        doc_string = re.sub('\\n+', ' ', doc_string)\n",
    "        doc_string = re.sub(' +', ' ', doc_string)\n",
    "        doc_string = doc_string.strip()\n",
    "        if doc_string == '' or doc_string == ' ':\n",
    "            doc_string = '$$$STUB$$$'\n",
    "\n",
    "        return doc_string\n",
    "\n",
    "    def prerproc_docs(self, docs, n_jobs, remove_stub_strings=True):\n",
    "        preprocessed_docs = Parallel(n_jobs, verbose=10)(delayed(self.preproc_doc_string)(doc) for doc in docs)\n",
    "        #docs = np.hstack(docs)\n",
    "\n",
    "        return preprocessed_docs\n",
    "\n",
    "    def get_uniq_text_list(self, preproc_texts):\n",
    "        uniq_texts = np.hstack([preproc_texts[:, 0], preproc_texts[:, 1]])\n",
    "        uniq_texts = np.unique(uniq_texts)\n",
    "        uniq_texts = list(sorted(list(uniq_texts)))\n",
    "        return uniq_texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0fdf4b-aa1f-4086-a405-e5e6fd169c01",
   "metadata": {},
   "source": [
    "TfidfW2vVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc6c1a5-2046-45bd-8a9a-1b780bffb924",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "class TfidfW2vVectorizer():\n",
    "    def __init__(self):\n",
    "        self.w2vModel = None\n",
    "        self.w2v_dict = None\n",
    "        self.tfidf_vectorizer = TfidfVectorizer()  # kaggle_all_the_news\n",
    "        # self.tfidf_vectorizer = TfidfVectorizer(tokenizer=identity_tokenizer,\n",
    "        # lowercase=False, stop_words=None)  # kaggle_survey_2020\n",
    "        pass\n",
    "    \n",
    "    def fit(self, corpus, vector_size=100, window=5,\n",
    "            n_jobs=10, min_count=2, sample=1e-5, epochs=10, sg=0, seed=45):\n",
    "        \n",
    "        corpus = SimpleDataPreprocessor().prerproc_docs(corpus, n_jobs, remove_stub_strings=False)\n",
    "        \n",
    "        print('Calculating TF-IDF weights')\n",
    "        self.tfidf_vectorizer.fit(corpus)\n",
    "        \n",
    "        print('Fitting W2V model')\n",
    "        self.make_w2v_dict(corpus, vector_size=vector_size, window=window, n_jobs=n_jobs,\n",
    "                                    min_count=min_count, sample=sample, epochs=epochs, sg=sg, seed=seed)\n",
    "        self.cache_vectors = None\n",
    "        self.use_cache_vectors = False\n",
    "        return self\n",
    "\n",
    "    def make_w2v_dict(self, docs, vector_size=128, window=5, n_jobs=10,\n",
    "                    min_count=1, sample=0, epochs=100, sg=0, seed=45):\n",
    "\n",
    "        docs = list(docs)\n",
    "        for i in range(len(docs)):\n",
    "            docs[i] = docs[i].split()\n",
    "\n",
    "        self.w2vModel = Word2Vec(\n",
    "            docs,\n",
    "            vector_size=vector_size,\n",
    "            window=window,\n",
    "            workers=n_jobs,\n",
    "            min_count=min_count,\n",
    "            sample=sample,\n",
    "            epochs=epochs,\n",
    "            sg=sg,\n",
    "            seed=seed)\n",
    "        self.w2v_dict = dict(\n",
    "            zip(self.w2vModel.wv.index_to_key, self.w2vModel.wv.vectors))\n",
    "        docs = None\n",
    "        gc.collect()\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def vectorize_docs(self, docs, use_tfidf=True, n_jobs=8):\n",
    "        \n",
    "        def vectorize_batch(docs, tfidf_vectorizer, w2v_dict, verbose=True, use_tfidf=True):\n",
    "            docs = list(docs)\n",
    "            tfidf_feats = tfidf_vectorizer.transform(docs)\n",
    "            for i in range(len(docs)):\n",
    "                docs[i] = docs[i].split()\n",
    "    \n",
    "            doc_vectors = []\n",
    "            if verbose:\n",
    "                proc_range = tqdm(range(len(docs)), desc='Vectorizing docs')\n",
    "            else:\n",
    "                proc_range = range(len(docs))\n",
    "    \n",
    "            tfidf_vocab = tfidf_vectorizer.vocabulary_\n",
    "            for i in proc_range:\n",
    "                tmp_vector = []\n",
    "                sentence_tfidf = tfidf_feats[i].toarray()\n",
    "                for j in range(len(docs[i])):\n",
    "                    if docs[i][j] in w2v_dict:\n",
    "                        if use_tfidf:\n",
    "                            if docs[i][j] not in tfidf_vocab:\n",
    "                                continue\n",
    "                            tfidfInd = tfidf_vocab[docs[i][j]]\n",
    "                            tfidf = sentence_tfidf[0][tfidfInd]\n",
    "                            tmp_vector.append(tfidf * w2v_dict[docs[i][j]])\n",
    "                        else:\n",
    "                            tmp_vector.append(w2v_dict[docs[i][j]])\n",
    "                if len(tmp_vector) != 0:\n",
    "                    tmp_vector = np.array(tmp_vector)\n",
    "                    tmp_vector = np.mean(tmp_vector, axis=0)\n",
    "                else:\n",
    "                    tmp_vector = np.zeros(list(w2v_dict.values())[0].shape)\n",
    "                doc_vectors.append(tmp_vector)\n",
    "            return doc_vectors\n",
    "        \n",
    "        \n",
    "        docs = SimpleDataPreprocessor().prerproc_docs(docs, n_jobs, remove_stub_strings=False)\n",
    "        \n",
    "        tfidf_vectorizers = []\n",
    "        w2v_dicts = []\n",
    "        for i in range( n_jobs ):\n",
    "            tfidf_vectorizers.append( deepcopy(self.tfidf_vectorizer) )\n",
    "            w2v_dicts.append( deepcopy(self.w2v_dict) )\n",
    "        \n",
    "        doc_batches = []\n",
    "        batch_size = len(docs) // n_jobs\n",
    "        for i in range(n_jobs - 1):\n",
    "            doc_batches.append( docs[i*batch_size : (i+1)*batch_size] )\n",
    "        doc_batches.append( docs[(n_jobs-1)*batch_size:] )\n",
    "        \n",
    "        vectorized_docs = Parallel(n_jobs, verbose=10)(delayed(vectorize_batch)\\\n",
    "                                                       (doc, tfidf_vectorizer, w2v_dict, verbose=True, use_tfidf=use_tfidf) \\\n",
    "                                                       for doc, tfidf_vectorizer, w2v_dict  \\\n",
    "                                                       in zip(doc_batches, tfidf_vectorizers, w2v_dicts))\n",
    "        del doc_batches\n",
    "        gc.collect()\n",
    "        \n",
    "        vectorized_docs = np.vstack( vectorized_docs )\n",
    "        \n",
    "        return vectorized_docs\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9bb333-9c28-4aa3-9809-3016462cd288",
   "metadata": {},
   "source": [
    "ProfitTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e209bbc7-f50b-4091-a56f-37b13be20d9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ProfitTM():\n",
    "\n",
    "    def __init__(self, SVM_C=1.0, n_jobs=10, verbose=1, name=None):\n",
    "\n",
    "        self.feature_extractor = CenterLossCompressor()\n",
    "        self.classifier = SGDClassifier(n_jobs=n_jobs)\n",
    "        self.topic_names = None\n",
    "        self.topic_count = None\n",
    "        self.name = name\n",
    "\n",
    "        pass\n",
    "    \n",
    "    def fit(self, x, max_agg_elements=30000, target_n_clusters=20, opt_param_dev=0.0, n_optimal_steps=1,\n",
    "            batch_size=20, base_epochs=25):\n",
    "        \n",
    "        if not isinstance(x, np.ndarray):\n",
    "            x = np.array( x )\n",
    "        \n",
    "        x = x.copy()\n",
    "        \n",
    "        if len(x) <= target_n_clusters:\n",
    "            target_n_clusters = 1\n",
    "        if len(x) == 1:\n",
    "            x = np.vstack([x, x])\n",
    "        \n",
    "        if len(x) > max_agg_elements:\n",
    "            np.random.seed(45)\n",
    "            subsample_ids = np.random.choice( list(range(len(x))), size=max_agg_elements, replace=False )\n",
    "            if not isinstance(subsample_ids, np.ndarray):\n",
    "                subsample_ids = np.array( subsample_ids )\n",
    "            x = x[subsample_ids]\n",
    "        \n",
    "        self.clusterizer = AgglomerativeClustering(n_clusters=target_n_clusters, linkage='ward')\n",
    "        self.clusterizer.fit(x)\n",
    "        clust_train_y = self.clusterizer.labels_\n",
    "        \n",
    "        optimal_clust_train_y = self.distance_bazed_cluster_merge(x, clust_train_y, metric='cosine', n_quantiles=20)\n",
    "        \n",
    "        if len(np.unique(optimal_clust_train_y)) == 1:\n",
    "            self.classifier = StubClassifier(stub_y=optimal_clust_train_y[0])\n",
    "            print(\"Optimal clusters by distance merge is 1. Place stub 1 class classifier.\")\n",
    "            return self\n",
    "            pass\n",
    "        else:\n",
    "            clust_train_y = optimal_clust_train_y\n",
    "\n",
    "        self.feature_extractor.fit(\n",
    "            x,\n",
    "            clust_train_y,\n",
    "            batch_size=batch_size,\n",
    "            epochs=base_epochs)\n",
    "        x_compressor_features = self.feature_extractor.predict(x)\n",
    "        self.classifier.fit(x_compressor_features, clust_train_y)\n",
    "\n",
    "        clust_train_y = self.classifier.predict(x_compressor_features)\n",
    "        optimal_clust_train_y = self.size_bazed_cluster_merge(x_compressor_features, clust_train_y, small_cluster_threshold=0.04, n_quantiles=20)\n",
    "            \n",
    "        if len(np.unique(optimal_clust_train_y)) == 1:\n",
    "            #self.classifier = StubClassifier(stub_y=optimal_clust_train_y[0])\n",
    "            print(\"Skip cluster size merge\")\n",
    "            return self\n",
    "            #pass\n",
    "        else:\n",
    "            clust_train_y = optimal_clust_train_y\n",
    "        \n",
    "        \n",
    "        self.topic_count = len(np.unique(clust_train_y))\n",
    "        self.feature_extractor.fit(\n",
    "            x,\n",
    "            clust_train_y,\n",
    "            batch_size=batch_size,\n",
    "            epochs=base_epochs)\n",
    "        x_compressor_features = self.feature_extractor.predict(x)\n",
    "        self.classifier.fit(x_compressor_features, clust_train_y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_topic_names(self, x, current_level):\n",
    "\n",
    "        x = np.array(x)\n",
    "        topic_dict = {}\n",
    "        y = self.predict(x)\n",
    "        uniq_y = np.unique(y)\n",
    "        for i in range(len(uniq_y)):\n",
    "            clust_x = x[y == uniq_y[i]]\n",
    "            \n",
    "            topic_names = str(current_level) + \"_\" + str(uniq_y[i])\n",
    "            \n",
    "            topic_dict[uniq_y[i]] = topic_names\n",
    "\n",
    "        return topic_dict\n",
    "    \n",
    "    def get_features(self, x):\n",
    "        \n",
    "        estimates = self.feature_extractor.predict(x)\n",
    "        return estimates\n",
    "\n",
    "    def get_class_estimates(self, x):\n",
    "        features = self.get_features(x)\n",
    "        estimates = self.classifier.decision_function(features)\n",
    "        return estimates\n",
    "\n",
    "    def predict(self, x):\n",
    "        \n",
    "        if isinstance(self.classifier, StubClassifier):\n",
    "            pred_y = self.classifier.predict(x)\n",
    "        else:\n",
    "            estimates = self.get_features(x)\n",
    "            pred_y = self.classifier.predict(estimates)\n",
    "            \n",
    "        return pred_y\n",
    "\n",
    "    \n",
    "    def find_optimal_clusters(\n",
    "            self, x, target_n_clusters, target_n_clustersDev, n_optimal_steps):\n",
    "\n",
    "        start_n_clusters = int((target_n_clustersDev) * target_n_clusters) + 1\n",
    "        end_n_clusters = int((1 + (1 - target_n_clustersDev)) * target_n_clusters)\n",
    "        n_clust = np.linspace(\n",
    "            start_n_clusters,\n",
    "            end_n_clusters,\n",
    "            n_optimal_steps,\n",
    "            dtype=int)\n",
    "        best_clusterizer = None\n",
    "        best_score = -1e20\n",
    "        best_n_clusters = None\n",
    "        for i in range(len(n_clust)):\n",
    "            clusterizer = AgglomerativeClustering(\n",
    "                n_clusters=n_clust[i], linkage='ward')\n",
    "            clusterizer.fit(x)\n",
    "            labels = clusterizer.labels_\n",
    "            if len(np.unique(labels)) == 1:\n",
    "                best_clusterizer = deepcopy(clusterizer)\n",
    "                best_n_clusters = n_clust[i]\n",
    "                break\n",
    "            score = calinski_harabasz_score(x, labels=labels)\n",
    "            print(\n",
    "                '{} | n_clusters = {} | score = {}'.format(\n",
    "                    'Stub', n_clust[i], score))\n",
    "            if score > best_score:\n",
    "                best_clusterizer = deepcopy(clusterizer)\n",
    "                best_score = score\n",
    "                best_n_clusters = n_clust[i]\n",
    "        print('Best score at {}: {}'.format(best_n_clusters, best_score))\n",
    "\n",
    "        self.clusterizer = deepcopy(best_clusterizer)\n",
    "        pass\n",
    "\n",
    "    def size_bazed_cluster_merge(\n",
    "            self, x, y, small_cluster_threshold=0.04, n_quantiles=20):\n",
    "        \n",
    "        y = y.copy()\n",
    "        # get size threshold\n",
    "        x = np.array(x)\n",
    "        uniq_y = np.unique(y)\n",
    "        clust_sizes = []\n",
    "        for i in range(len(uniq_y)):\n",
    "            clust_x = x[y == uniq_y[i]]\n",
    "            clust_size = len(clust_x)\n",
    "            clust_sizes.append(clust_size)\n",
    "        clust_sizes = np.array(clust_sizes)\n",
    "        size_quantiles = pd.DataFrame({'clust_size': clust_sizes})\n",
    "        print('size quantiles')\n",
    "        pprint(\n",
    "            pd.qcut(\n",
    "                size_quantiles['clust_size'],\n",
    "                n_quantiles,\n",
    "                duplicates='drop').value_counts().index)\n",
    "        size_quantiles = list(\n",
    "            sorted(\n",
    "                list(\n",
    "                    pd.qcut(\n",
    "                        size_quantiles['clust_size'],\n",
    "                        n_quantiles,\n",
    "                        duplicates='drop').value_counts().index)))\n",
    "        #size_quantiles[0].left = abs(size_quantiles[0].left)\n",
    "\n",
    "        size_threshold = None\n",
    "        relative_borders = []\n",
    "        # last max change can be at the end of sorted quantiles\n",
    "        # define optimal threshold as the max change\n",
    "        for i in range(len(size_quantiles) - 1):\n",
    "            relative_border = size_quantiles[i].right / abs(size_quantiles[i].left)\n",
    "            relative_borders.append(relative_border)\n",
    "        # if no changes then don't merge\n",
    "        if len(relative_borders) == 0:\n",
    "            return y\n",
    "        max_relative_border_id = np.argmax(relative_borders)\n",
    "        max_relative_border = relative_borders[max_relative_border_id]\n",
    "        # if there was no big change between sorted quantile sizes\n",
    "        # then there are no small trash clusters\n",
    "        print('Max relative border: {}'.format(max_relative_border))\n",
    "        size_threshold = size_quantiles[max_relative_border_id].right\n",
    "\n",
    "        # find small clusters\n",
    "        merge_dict = {}\n",
    "        max_cluster_size = max(clust_sizes)\n",
    "        relative_sizes = []\n",
    "        for i in range(len(uniq_y)):\n",
    "            clust_x = x[y == uniq_y[i]]\n",
    "            clust_size = len(clust_x)\n",
    "            relative_size = clust_size / max_cluster_size\n",
    "            relative_sizes.append(relative_size)\n",
    "            if clust_size <= size_threshold:\n",
    "                # if relative_size <= small_cluster_threshold:\n",
    "                merge_dict[uniq_y[i]] = []\n",
    "\n",
    "        print('relative sizes')\n",
    "        pprint(relative_sizes)\n",
    "        print('Max cluster size: {}'.format(max_cluster_size))\n",
    "        # find nearest big cluster for small cluster\n",
    "        for small_cluster_id in merge_dict.keys():\n",
    "            small_cluster = x[y == small_cluster_id]\n",
    "            small_cluster_center = np.mean(small_cluster, axis=0)\n",
    "            min_dist = 1e30\n",
    "            best_big_cluser_id = None\n",
    "            for big_cluster_id in uniq_y:\n",
    "                if small_cluster_id == big_cluster_id:\n",
    "                    continue\n",
    "\n",
    "                big_cluster = x[y == big_cluster_id]\n",
    "                big_cluster_size = len(big_cluster)\n",
    "                if big_cluster_size <= size_threshold:  # don't merge with other small\n",
    "                    continue\n",
    "\n",
    "                big_cluster_center = np.mean(big_cluster, axis=0)\n",
    "                dist = cosine(small_cluster_center, big_cluster_center)\n",
    "                if dist < min_dist:\n",
    "                    best_big_cluser_id = big_cluster_id\n",
    "            merge_dict[small_cluster_id].append(best_big_cluser_id)\n",
    "        pprint(merge_dict)\n",
    "\n",
    "        optimal_y = self.merge_clusters(y, merge_dict)\n",
    "        return optimal_y\n",
    "\n",
    "    def distance_bazed_cluster_merge(\n",
    "            self, x, y, metric='cosine', n_quantiles=20):\n",
    "            \n",
    "        # get centers of each cluster as mean of top N words closest to center\n",
    "        y = y.copy()\n",
    "        x = np.array(x)\n",
    "        uniq_y = np.unique(y)\n",
    "        centers = []\n",
    "        for i in range(len(uniq_y)):\n",
    "            clust_x = x[y == uniq_y[i]]\n",
    "            clusters_center = np.mean(clust_x, axis=0)\n",
    "            centers.append(clusters_center)\n",
    "        centers = np.array(centers)\n",
    "\n",
    "        # get distance threshold for merging\n",
    "        distances = []\n",
    "        for i in range(len(centers)):\n",
    "            for j in range(len(centers)):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                if metric == 'cosine':\n",
    "                    dist = cosine(centers[i], centers[j])\n",
    "                else:\n",
    "                    dist = euclidean(centers[i], centers[j])\n",
    "                distances.append(dist) \n",
    "        if len(distances) == 0:\n",
    "            return y\n",
    "        distances = pd.DataFrame({'dist': distances})\n",
    "        \n",
    "        pprint(pd.qcut(distances['dist'], n_quantiles, duplicates='drop'))\n",
    "        distance_quantiles = list(\n",
    "            sorted(\n",
    "                list(\n",
    "                    pd.qcut(\n",
    "                        distances['dist'],\n",
    "                        n_quantiles,\n",
    "                        duplicates='drop').value_counts().index)))\n",
    "\n",
    "        # if no quantiles then don't merge\n",
    "        if len(distance_quantiles) == 0:\n",
    "            return y\n",
    "\n",
    "        #####################################\n",
    "        distance_threshold = None\n",
    "        relative_borders = []\n",
    "        # last max change can be at the end of sorted quantiles\n",
    "        # define optimal threshold as the max change\n",
    "        for i in range(len(distance_quantiles) - 1):\n",
    "            relative_border = distance_quantiles[i].right / abs(distance_quantiles[i].left)\n",
    "            relative_borders.append(relative_border)\n",
    "        # if no changes then don't merge\n",
    "        if len(relative_borders) == 0:\n",
    "            return y\n",
    "        max_relative_border_id = np.argmax(relative_borders)\n",
    "        max_relative_border = relative_borders[max_relative_border_id]\n",
    "        # if there was no big change between sorted quantile sizes\n",
    "        # then there are no small trash clusters\n",
    "        print('Max relative distance border: {}'.format(max_relative_border))\n",
    "        distance_threshold = distance_quantiles[max_relative_border_id].right\n",
    "        #####################################\n",
    "\n",
    "        #####################################\n",
    "        #distance_threshold = distance_quantiles[0].right\n",
    "        #####################################\n",
    "\n",
    "        # get clusters which centers are closer than distance threshold\n",
    "        merge_dict = {}\n",
    "        for i in range(len(centers)):\n",
    "            merge_dict[uniq_y[i]] = []\n",
    "            for j in range(len(centers)):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                if metric == 'cosine':\n",
    "                    dist = cosine(centers[i], centers[j])\n",
    "                else:\n",
    "                    dist = euclidean(centers[i], centers[j])\n",
    "                if dist <= distance_threshold:\n",
    "                    merge_dict[uniq_y[i]].append(uniq_y[j])\n",
    "            if len(merge_dict[uniq_y[i]]) == 0:\n",
    "                merge_dict[uniq_y[i]].append(-1)\n",
    "\n",
    "        optimal_y = self.merge_clusters(y, merge_dict)\n",
    "        return optimal_y\n",
    "\n",
    "    def merge_clusters(self, y, merge_dict):\n",
    "        # get initial merge components list\n",
    "        merge_list = []\n",
    "        for key in merge_dict.keys():\n",
    "            if -1 not in merge_dict[key]:\n",
    "                merge_component = []\n",
    "                merge_component.append(key)\n",
    "                for cluster_to_merge in merge_dict[key]:\n",
    "                    merge_component.append(cluster_to_merge)\n",
    "                merge_component = list(sorted(merge_component))\n",
    "                merge_list.append(merge_component)\n",
    "        uniq_components = []\n",
    "        for merge_component in merge_list:\n",
    "            if merge_component not in uniq_components:\n",
    "                uniq_components.append(merge_component)\n",
    "        merge_list = uniq_components\n",
    "        merge_list = list(sorted(merge_list, key=len, reverse=True))\n",
    "        print(merge_list)\n",
    "\n",
    "        # clean merge list\n",
    "        components_lens = set()\n",
    "        for merge_component in merge_list:\n",
    "            components_lens.add(len(merge_component))\n",
    "        components_lens = list(sorted(list(components_lens), reverse=True))\n",
    "        for k in range(len(components_lens[:len(components_lens) - 1])):\n",
    "            target_components = []\n",
    "            target_idx = []\n",
    "            sub_components = []\n",
    "            subset_idx = []\n",
    "            for i in range(len(merge_list)):\n",
    "                if len(merge_list[i]) == components_lens[k]:\n",
    "                    target_components.append(set(merge_list[i]))\n",
    "                    target_idx.append(i)\n",
    "                elif len(merge_list[i]) <= 1:\n",
    "                    pass\n",
    "                elif len(merge_list[i]) in components_lens[k + 1:]:\n",
    "                    sub_components.append(set(merge_list[i]))\n",
    "            for i in range(len(target_components)):\n",
    "                for j in range(len(sub_components)):\n",
    "                    intersect = sub_components[j].intersection(target_components[i])\n",
    "                    if sub_components[j] == intersect:\n",
    "                        subset_idx.append(j)\n",
    "            subset_idx = list(set(subset_idx))\n",
    "            for i in range(len(target_components)):\n",
    "                for j in range(len(subset_idx)):\n",
    "                    target_components[i] = target_components[i].difference(\n",
    "                        sub_components[subset_idx[j]])\n",
    "            for i in range(len(target_idx)):\n",
    "                merge_list[target_idx[i]] = target_components[i]\n",
    "\n",
    "        for i in range(len(merge_list)):\n",
    "            merge_list[i] = set(merge_list[i])\n",
    "        tmp = []\n",
    "        for i in range(len(merge_list)):\n",
    "            if merge_list[i] not in tmp and len(merge_list[i]) > 1:\n",
    "                tmp.append(merge_list[i])\n",
    "        merge_list = tmp\n",
    "        print(merge_list)\n",
    "\n",
    "        # get final clean components\n",
    "        clean_components = []\n",
    "        for i in range(len(merge_list)):\n",
    "            current_component = set()\n",
    "            for j in range(i, len(merge_list)):\n",
    "                if current_component == set():\n",
    "                    current_component = current_component.union(merge_list[j])\n",
    "                else:\n",
    "                    if current_component.intersection(merge_list[j]) != set():\n",
    "                        current_component = current_component.union(\n",
    "                            merge_list[j])\n",
    "            not_in_flag = True\n",
    "            for i in range(len(clean_components)):\n",
    "                if current_component.intersection(\n",
    "                        clean_components[i]) != set():\n",
    "                    not_in_flag = False\n",
    "            if not_in_flag:\n",
    "                clean_components.append(current_component)\n",
    "        print(clean_components)\n",
    "\n",
    "        # set new labels\n",
    "        for opt_set_y in clean_components:\n",
    "            min_y = min(opt_set_y)\n",
    "            for i in range(len(y)):\n",
    "                if y[i] in opt_set_y:\n",
    "                    y[i] = min_y\n",
    "        uniq_y = np.unique(y)\n",
    "        old_new_dict = {}\n",
    "        for i in range(len(uniq_y)):\n",
    "            old_new_dict[uniq_y[i]] = i\n",
    "        for i in range(len(y)):\n",
    "            y[i] = old_new_dict[y[i]]\n",
    "        optimal_y = y\n",
    "\n",
    "        uniq_y = np.unique(optimal_y)\n",
    "        print('Optimal topics count = {}'.format(len(uniq_y)))\n",
    "\n",
    "        return optimal_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8d8205-a9c3-45d9-8a7a-dc4d1c3961d5",
   "metadata": {},
   "source": [
    "TreeProfitTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544c21ca-d12b-4724-b152-a9c2a567681a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class TreeProfitTM():\n",
    "\n",
    "    def __init__(self, max_depth=None, current_level=0, parents_name=None):\n",
    "        self.node = None\n",
    "        self.childs = {}\n",
    "        self.topic_names = None\n",
    "        self.topic_count = None\n",
    "        self.max_depth = max_depth\n",
    "        self.current_level = current_level\n",
    "        self.tree_name = str(uuid.uuid4())\n",
    "\n",
    "        if current_level == 0 and parents_name is None:\n",
    "            self.isRoot = True\n",
    "        else:\n",
    "            self.isRoot = False\n",
    "\n",
    "        pass\n",
    "\n",
    "    def fit(self, x):\n",
    "\n",
    "        if self.current_level == 0:\n",
    "            self.node = ProfitTM()\n",
    "            self.node.fit(x)\n",
    "        else:\n",
    "            self.node.fit(x)\n",
    "        \n",
    "        self.topic_names = self.node.get_topic_names(x, current_level=self.current_level)\n",
    "        self.topic_count = self.node.topic_count\n",
    "\n",
    "        if self.current_level + 1 < self.max_depth:\n",
    "            y = self.node.predict(x)\n",
    "            uniq_y = np.unique(y)\n",
    "            topic_docs = {}\n",
    "            for topic in uniq_y:\n",
    "\n",
    "                topic_docs[topic] = []\n",
    "                for i in range(len(y)):\n",
    "                    if y[i] == topic:\n",
    "                        topic_docs[topic].append(x[i])\n",
    "\n",
    "                self.childs[topic] = TreeProfitTM(self.max_depth, self.current_level + 1)\n",
    "                self.childs[topic].node = ProfitTM()\n",
    "\n",
    "        if self.current_level + 1 < self.max_depth:\n",
    "            for topic in topic_docs.keys():\n",
    "                self.childs[topic].fit(topic_docs[topic])\n",
    "        pass\n",
    "\n",
    "    def predict(self, x, return_vectors=False):\n",
    "\n",
    "        shared_predicts = self.prepare_to_predict(x)\n",
    "        all_row_ids = [i for i in range(len(x))]\n",
    "        all_row_ids = np.array( all_row_ids )\n",
    "        shared_predicts = self.hierarchical_predict(x, all_row_ids, shared_predicts, pred_ids=None)\n",
    "\n",
    "        if return_vectors:\n",
    "            shared_predicts = self.convert_predicts_to_vectors(shared_predicts)\n",
    "\n",
    "        return shared_predicts\n",
    "\n",
    "    def prepare_to_predict(self, x):\n",
    "        \n",
    "        shared_predicts = np.zeros( shape=(len(x), self.max_depth), dtype=np.int32 )\n",
    "        shared_predicts = shared_predicts + np.nan\n",
    "        \n",
    "        return shared_predicts\n",
    "\n",
    "    def hierarchical_predict(self, text_vectors, all_row_ids, shared_predicts, pred_ids=None):\n",
    "\n",
    "        if self.current_level < self.max_depth:\n",
    "\n",
    "            if pred_ids is None:\n",
    "                next_text_vectors_batch = text_vectors\n",
    "            else:\n",
    "                next_text_vectors_batch = text_vectors[pred_ids]\n",
    "            y = self.node.predict(next_text_vectors_batch)\n",
    "            \n",
    "            \n",
    "            if self.current_level == 0:\n",
    "                shared_predicts[:, 0] = y\n",
    "            else:\n",
    "                shared_predicts[pred_ids, self.current_level] = y\n",
    "                \n",
    "            uniq_y = np.unique(y)\n",
    "            for topic in uniq_y:\n",
    "                \n",
    "                if pred_ids is None:\n",
    "                    next_ids = all_row_ids[y == topic]\n",
    "                else:\n",
    "                    next_ids = all_row_ids[pred_ids][y == topic]\n",
    "                \n",
    "                if len(self.childs.keys()) == 0:\n",
    "                    #self.leaf_predict(text_vectors, shared_predicts, next_ids)\n",
    "                    pass\n",
    "                else:\n",
    "                    self.childs[topic].hierarchical_predict(text_vectors, all_row_ids, shared_predicts, next_ids)\n",
    "        \n",
    "        return shared_predicts\n",
    "    \n",
    "    def extract_features(self, x):\n",
    "\n",
    "        shared_predicts = self.prepare_to_feature_extraction(x)\n",
    "        all_row_ids = [i for i in range(len(x))]\n",
    "        all_row_ids = np.array( all_row_ids )\n",
    "        shared_predicts = self.hierarchical_feature_extraction(x, all_row_ids, shared_predicts, pred_ids=None)\n",
    "\n",
    "        return shared_predicts\n",
    "\n",
    "    def prepare_to_feature_extraction(self, x):\n",
    "        \n",
    "        shared_predicts = np.zeros( shape=(len(x), self.max_depth * self.node.feature_extractor.latent_dim), dtype=np.float64 )\n",
    "        shared_predicts = shared_predicts + np.nan\n",
    "        \n",
    "        return shared_predicts\n",
    "\n",
    "    def hierarchical_feature_extraction(self, text_vectors, all_row_ids, shared_predicts, pred_ids=None):\n",
    "\n",
    "        if self.current_level < self.max_depth:\n",
    "\n",
    "            if pred_ids is None:\n",
    "                next_text_vectors_batch = text_vectors\n",
    "            else:\n",
    "                next_text_vectors_batch = text_vectors[pred_ids]\n",
    "                \n",
    "            y = self.node.predict(next_text_vectors_batch)\n",
    "            topic_level_features = self.node.get_features(next_text_vectors_batch)\n",
    "            \n",
    "            insert_size = self.node.feature_extractor.latent_dim\n",
    "            if self.current_level == 0:\n",
    "                shared_predicts[:, : insert_size] = topic_level_features\n",
    "            else:\n",
    "                shared_predicts[pred_ids, self.current_level * insert_size : (self.current_level + 1) * insert_size] = topic_level_features\n",
    "                \n",
    "            uniq_y = np.unique(y)\n",
    "            for topic in uniq_y:\n",
    "                \n",
    "                if pred_ids is None:\n",
    "                    next_ids = all_row_ids[y == topic]\n",
    "                else:\n",
    "                    next_ids = all_row_ids[pred_ids][y == topic]\n",
    "                \n",
    "                if len(self.childs.keys()) == 0:\n",
    "                    #self.leaf_predict(text_vectors, shared_predicts, next_ids)\n",
    "                    pass\n",
    "                else:\n",
    "                    self.childs[topic].hierarchical_feature_extraction(text_vectors, all_row_ids, shared_predicts, next_ids)\n",
    "        \n",
    "        return shared_predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad0185d-979b-48ed-8c0d-e93da9b89622",
   "metadata": {},
   "source": [
    "TopicInterpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908dfb1f-021a-45b8-8040-00a0e596f0a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class TopicInterpreter():\n",
    "    \n",
    "    def __init__(self, vectorizer=None):\n",
    "        \n",
    "        self.vectorizer = vectorizer\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def fit(self, corpus, vector_size=100, window=5,\n",
    "            n_jobs=10, min_count=2, sample=1e-5, epochs=10, sg=0, seed=45):\n",
    "        \n",
    "        self.vectorizer = TfidfW2vVectorizer()\n",
    "        self.vectorizer.fit(corpus, vector_size, window, n_jobs, min_count, sample, epochs, sg, seed)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def make_topic_dict(self, text_vectors, topic_labels, level, n_jobs=8):\n",
    "        x = np.array(text_vectors)\n",
    "        topic_dict = {}\n",
    "        topic_labels = self.encode_topic_labels(topic_labels, level)\n",
    "        y = topic_labels\n",
    "        uniq_y = np.unique(y)\n",
    "        centers = []\n",
    "        for i in range(len(uniq_y)):\n",
    "            clust_x = x[y == uniq_y[i]]\n",
    "            clusters_center = np.mean(clust_x, axis=0)\n",
    "            centers.append(clusters_center)\n",
    "            most_similar = self.vectorizer.w2vModel.wv.most_similar(positive=clusters_center, topn=5)\n",
    "            \n",
    "            topic_name = []\n",
    "            for j in range(len(most_similar)):\n",
    "                topic_name_part = most_similar[j][0]\n",
    "                topic_name.append( topic_name_part )\n",
    "            topic_name = \" \".join(topic_name)\n",
    "            \n",
    "            topic_dict[uniq_y[i]] = topic_name\n",
    "\n",
    "        return topic_dict\n",
    "    \n",
    "    def get_topic_names(self, texts, topic_labels, level, n_jobs=8):\n",
    "        \n",
    "        x = self.vectorizer.vectorize_docs(texts, use_tfidf=True, n_jobs=n_jobs)\n",
    "        topic_dict = self.make_topic_dict(x, topic_labels, level, n_jobs=8)\n",
    "\n",
    "        return topic_dict\n",
    "    \n",
    "    def encode_topic_labels(self, topic_labels, level=None):\n",
    "        \n",
    "        topic_labels = topic_labels.copy()\n",
    "        \n",
    "        if level is None:\n",
    "            level = topic_labels.shape[1] - 1\n",
    "        \n",
    "        merged_labels = []\n",
    "        for i in range(len(topic_labels)):\n",
    "            merged_label = []\n",
    "            for j in range(level+1):\n",
    "                merged_label.append(str(topic_labels[i][j])) \n",
    "            merged_label = \"_\".join(merged_label)\n",
    "            \n",
    "            merged_labels.append(merged_label)\n",
    "        \n",
    "        merged_labels = np.array( merged_labels )\n",
    "        \n",
    "        return merged_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa6639a-1850-4836-b541-3ee0dbde47f5",
   "metadata": {},
   "source": [
    "Так как ноут демонстрационный, все чтение и сохрание данных происходит в рамках одной папки. Поместите тестовый .xlsx в одну папку с ноутом. Метки топиков будут лежать там же."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e1821d-2f2b-480c-913e-d6d39e6ca0fa",
   "metadata": {},
   "source": [
    "Чтение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db4b9fe-19a4-4ec2-8dd7-fec0b6112a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"articles1\"\n",
    "parsed_data = pd.read_excel( Path(\"data\", \"{}.xlsx\".format(file_name)), names=[\"text\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b76b60-6541-4ee0-b83e-d3727255829b",
   "metadata": {},
   "source": [
    "Тренировка векторизатора текстов (W2V + TF-IDF веса)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dc67b3-a5f4-45ae-85b6-47a155ceb634",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfW2vVectorizer()\n",
    "train_texts = list( set( parsed_data[\"text\"].to_list() ) )\n",
    "random.seed(45)\n",
    "random.shuffle( train_texts )\n",
    "vectorizer.fit(train_texts, vector_size=384, window=5,\n",
    "            n_jobs=8, min_count=2, sample=1e-5, epochs=100, sg=0, seed=45)\n",
    "save( vectorizer, Path(\".\", \"vectorizer.pkl\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f9e75-5ac1-4d1a-8e1f-ee70bf754f07",
   "metadata": {},
   "source": [
    "Получение векторов текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690800ab-946e-47fa-a202-75e58c763862",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = load( Path(\".\", \"vectorizer.pkl\") )\n",
    "texts_list = parsed_data[\"text\"].to_list()\n",
    "vectorized_texts = vectorizer.vectorize_docs(texts_list, use_tfidf=True, n_jobs=8)\n",
    "save( vectorized_texts, Path(\".\", \"vectorized_texts.pkl\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91daa11b-c799-4d2c-9358-3fb2baa1beb5",
   "metadata": {},
   "source": [
    "Тренировка тематической модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a57a5c-095c-438d-8d7c-463aa83f47e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_texts = load( Path(\".\", \"vectorized_texts.pkl\") )\n",
    "topic_model = TreeProfitTM( max_depth=2 )\n",
    "topic_model.fit(vectorized_texts)\n",
    "save( topic_model, Path(\".\", \"topic_model.pkl\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d4d508-af33-46d2-9411-4a22371b6375",
   "metadata": {},
   "source": [
    "Получение меток топиков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff83da6-3dc0-40cb-9fe3-77119bb01e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = load( Path( \".\", \"topic_model.pkl\") )\n",
    "vectorized_texts = load( Path(\".\", \"vectorized_texts.pkl\") )\n",
    "topic_labels = topic_model.predict( vectorized_texts, return_vectors = False )\n",
    "save( pred_y, Path(\".\", \"topic_labels.pkl\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423d2911-8661-4cbf-8197-caa4a9011d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_interpreter = TopicInterpreter()\n",
    "topic_interpreter.fit(texts_list, vector_size=384, window=5, n_jobs=8, min_count=2, sample=1e-5, epochs=100, sg=0, seed=45)\n",
    "save( topic_interpreter, Path(\".\", \"topic_interpreter.pkl\") )\n",
    "\n",
    "topic_names_0 = topic_interpreter.get_topic_names(texts_list, topic_labels, level=0, n_jobs=8)\n",
    "pprint( topic_names_0 )\n",
    "topic_names_1 = topic_interpreter.get_topic_names(texts_list, topic_labels, level=1, n_jobs=8)\n",
    "pprint( topic_names_1 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
